{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99d70d15-54a9-4880-90ae-b888d311b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import collections\n",
    "\n",
    "from exp_setup import *\n",
    "from coprover.results_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c238a0a-702e-4737-9c9b-6af173bb2bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_fpath = Path(\"results\", \"t5_compare_pred.v1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31766671-0e68-4e37-b105-1a3cd72d7481",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_gg_telem = read_gg_from_csv(res_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ed02770-882a-4300-9bf0-3ebfd5a3eb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           n       0.94      0.91      0.92      5475\n",
      "           p       0.91      0.94      0.92      5460\n",
      "\n",
      "    accuracy                           0.92     10935\n",
      "   macro avg       0.92      0.92      0.92     10935\n",
      "weighted avg       0.92      0.92      0.92     10935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(v1_gg_telem.class_report(return_dict=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e96e6b0b-14cd-41d3-8382-d254cc264c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap, # samples=1000, acc/std=0.92314/0.00249\n"
     ]
    }
   ],
   "source": [
    "mean_acc, std_acc, samples = bootstrap_acc_stats(res_fpath)\n",
    "print(f\"Bootstrap, # samples={samples}, acc/std={mean_acc:.5f}/{std_acc:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70583a65-fda3-4c2f-8505-88c3d8ad06bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fennel1/yeh/anaconda/anaconda3/envs/coprovers/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/fennel1/yeh/anaconda/anaconda3/envs/coprovers/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/fennel1/yeh/anaconda/anaconda3/envs/coprovers/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           n       0.00      0.00      0.00      5475\n",
      "           p       0.50      1.00      0.67      5460\n",
      "\n",
      "    accuracy                           0.50     10935\n",
      "   macro avg       0.25      0.50      0.33     10935\n",
      "weighted avg       0.25      0.50      0.33     10935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline_gg_telem = read_gg_from_csv(res_fpath)\n",
    "baseline_gg_telem.guesses = [POS for _ in range(len(baseline_gg_telem.guesses))]\n",
    "print(baseline_gg_telem.class_report(return_dict=False))\n",
    "baseline_gg_telem.save(Path(\"results\", \"baseline.v1.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86f2d29b-0ae3-443e-82dd-029a3ffc233f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap baseline, # samples=1000, acc/std=0.49936/0.00481\n"
     ]
    }
   ],
   "source": [
    "baseline_mean_acc, baseline_std_acc, baseline_samples = bootstrap_acc_stats(Path(\"results\", \"baseline.v1.csv\"))\n",
    "print(f\"Bootstrap baseline, # samples={baseline_samples}, acc/std={baseline_mean_acc:.5f}/{baseline_std_acc:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0e217f2a-9389-4264-bb9c-c453651cae1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PVal=8179.092100435865\n",
      "Critical value = 3.291/-3.291, two tailed alpha=0.001, N=10935\n"
     ]
    }
   ],
   "source": [
    "p_val = two_t(mean_acc, std_acc, len(test_df), baseline_mean_acc, baseline_std_acc, len(test_df))\n",
    "print(f\"PVal={p_val}\")\n",
    "cv1, cv2 = get_crit_vals(0.001, len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74d57cc9-1684-48ae-854e-c0e379703ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up last step pred from compare_pred.v1.csv.gz\n",
      "Len train=98411, test=10935\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = setup_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43bc1812-1158-4672-84a2-bb31a7a34e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'p': 49213, 'n': 49198})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(train_df.target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a09c84f-638f-4a59-97e5-cd04123dd60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'n': 5475, 'p': 5460})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(test_df.target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5fb7342-d13c-4d2c-af5c-6b4e9ec059a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98411/98411 [00:10<00:00, 9126.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'LT': 42764, 'GT': 42623, 'EQ': 13024})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: Check for token lengths for sequents\n",
    "\n",
    "s1_lt_s2s = []\n",
    "for _, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
    "    (cmds, s1, s2) = row.source_text.split(\"<ANT>\")\n",
    "    toks_s1 = s1.strip().split()\n",
    "    toks_s2 = s2.strip().split()\n",
    "    if len(toks_s1) < len(toks_s2):\n",
    "        s1_lt_s2s.append(\"LT\")\n",
    "    elif len(toks_s1) > len(toks_s2):\n",
    "        s1_lt_s2s.append(\"GT\")\n",
    "    else:\n",
    "        s1_lt_s2s.append(\"EQ\")\n",
    "print(collections.Counter(s1_lt_s2s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cc79a7c-06bc-4c6c-9411-7f30a776dab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10935/10935 [00:01<00:00, 8480.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'GT': 4823, 'LT': 4682, 'EQ': 1430})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: Check for token lengths for sequents\n",
    "\n",
    "s1_lt_s2s = []\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    (cmds, s1, s2) = row.source_text.split(\"<ANT>\")\n",
    "    toks_s1 = s1.strip().split()\n",
    "    toks_s2 = s2.strip().split()\n",
    "    if len(toks_s1) < len(toks_s2):\n",
    "        s1_lt_s2s.append(\"LT\")\n",
    "    elif len(toks_s1) > len(toks_s2):\n",
    "        s1_lt_s2s.append(\"GT\")\n",
    "    else:\n",
    "        s1_lt_s2s.append(\"EQ\")\n",
    "print(collections.Counter(s1_lt_s2s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coprovers] *",
   "language": "python",
   "name": "conda-env-coprovers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
