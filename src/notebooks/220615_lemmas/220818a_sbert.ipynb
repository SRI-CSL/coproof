{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b87640a3-d4cc-4031-90cd-a438263fc578",
   "metadata": {},
   "source": [
    "This sets up a SBert instance using the Roberta model trained with 220818_train_llm and 220817_create_corpus\n",
    "\n",
    "We develop a corpus from the train/test lemmas and queries.\n",
    "\n",
    "Note that currently we seem to need to use a pre-trained BERT model for the training to work properly, as the custom vocabulary and pre-trained objective does not seem to be working well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d7f0f0c-3136-41cf-b344-f28600b10478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from transformers import RobertaTokenizerFast, AutoModel\n",
    "from setup_queries import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d8eee34-072f-4a24-a82f-9e597c7a5afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_LIM=859\n",
    "MODEL_SAVE_DIR = Path(\"outputs/nir_sbert/\")\n",
    "MODEL_OUTPUT_PATH = Path(MODEL_SAVE_DIR, \"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c0baf21-e365-479e-bee8-4ed068d748e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"outputs/nir_vocab\", max_len=DOC_LIM)\n",
    "vocab_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3b69a17-a95b-4d90-8172-6b85063a2205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at outputs/nir_model/checkpoint-1100000 were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at outputs/nir_model/checkpoint-1100000 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "WORD_EMBED_DIM=768 # Obtained by model summary\n",
    "import sentence_transformers\n",
    "# word_embedding_model = AutoModel.from_pretrained(\"outputs/nir_model/checkpoint-1100000\")\n",
    "word_embedding_model = sentence_transformers.models.Transformer(\"outputs/nir_model/checkpoint-1100000\",\n",
    "                                                               tokenizer_name_or_path=\"outputs/nir_vocab\",\n",
    "                                                               max_seq_length=DOC_LIM)\n",
    "pooling_model = models.Pooling(WORD_EMBED_DIM)\n",
    "# word_embedding_model.tokenizer = tokenizer.tokenize # Set the function\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0d6ccd-d3e9-4fde-835b-753dd7c24889",
   "metadata": {},
   "source": [
    "Now set up a set of SentBERT InputExample entries for contrastive learning\n",
    "We first generate a sample set from the set of theories and queries.  We first\n",
    "train a pure Siamese similarity function, and then assess the ranking ability of\n",
    "that against queries.  We then train a model specifically to do the asymmetric similarity assessment.\n",
    "\n",
    "First we use the theory bank and assemble an in-memory dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cee91c4-5fdb-4810-a68d-eb94d16f5a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "np.random.randint(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e55df8f-f783-4373-8167-ac9d4c6e4d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train=12132, test=8089, total=20221\n"
     ]
    }
   ],
   "source": [
    "# Read in Lemma requests\n",
    "import json\n",
    "with open(\"lemma_requests.json\", \"r\") as f:\n",
    "    lemma_requests = json.load(f)\n",
    "    \n",
    "    \n",
    "# Split into train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_queries, test_queries = train_test_split(\n",
    "     lemma_requests, train_size=0.6, random_state=1337, shuffle=True, stratify=None)\n",
    "print(\"# train={}, test={}, total={}\".format(len(train_queries), len(test_queries), len(lemma_requests)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5cd8e5c-a8bc-482a-8828-caf3ffe0fbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615 9205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 615/615 [00:02<00:00, 284.47it/s]\n"
     ]
    }
   ],
   "source": [
    "from theorybank import gen_default_theorybank\n",
    "\n",
    "theorybank = gen_default_theorybank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4d5ebc2-dbc9-47ba-9387-1c0005cd45cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the lemma requests.json, and use the lemma query and the lemma body in library as true positives.\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import InputExample, SentencesDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from setup_queries import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11bc83df-3e14-4b93-b485-6cc009a41af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the evaluator\n",
    "eval_set = test_examples\n",
    "sentences1 = [x.texts[0] for x in eval_set]\n",
    "sentences2 = [x.texts[1] for x in eval_set]\n",
    "labels = [x.label for x in eval_set]\n",
    "evaluator = sentence_transformers.evaluation.BinaryClassificationEvaluator(sentences1, sentences2, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58c3c169-4a62-426e-bc39-f41c24c91d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def status(score, epoch, steps):\n",
    "    print(\"Score={:.5f}, epoch={}, steps={}\".format(score, epoch, steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a992887d-cfea-42b3-9195-c6b1a8178bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f561286bb8ac48caa994ff277e9fad1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f536a0f487143e4a2e2d5a6e77988a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:300] . unexpected pos 26478848 vs 26478736",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda/anaconda3/envs/coprovers/lib/python3.9/site-packages/torch/serialization.py:379\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 379\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda/anaconda3/envs/coprovers/lib/python3.9/site-packages/torch/serialization.py:499\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    498\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m*\u001b[39m storage\u001b[38;5;241m.\u001b[39melement_size()\n\u001b[0;32m--> 499\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m losses\n\u001b[1;32m      3\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mCosineSimilarityLoss(model)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_objectives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m          \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m          \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mMODEL_OUTPUT_PATH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mMODEL_SAVE_DIR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_save_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_save_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/anaconda3/envs/coprovers/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:742\u001b[0m, in \u001b[0;36mSentenceTransformer.fit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[1;32m    739\u001b[0m                 loss_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    741\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m checkpoint_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m checkpoint_save_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m checkpoint_save_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m global_step \u001b[38;5;241m%\u001b[39m checkpoint_save_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 742\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_save_total_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_during_training(evaluator, output_path, save_best_model, epoch, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, callback)\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m output_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:   \u001b[38;5;66;03m#No evaluator, but output path: save final model version\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda/anaconda3/envs/coprovers/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:787\u001b[0m, in \u001b[0;36mSentenceTransformer._save_checkpoint\u001b[0;34m(self, checkpoint_path, checkpoint_save_total_limit, step)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_save_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, checkpoint_path, checkpoint_save_total_limit, step):\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# Store new checkpoint\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;66;03m# Delete old checkpoints\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m checkpoint_save_total_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m checkpoint_save_total_limit \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda/anaconda3/envs/coprovers/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:375\u001b[0m, in \u001b[0;36mSentenceTransformer.save\u001b[0;34m(self, path, model_name, create_model_card, train_datasets)\u001b[0m\n\u001b[1;32m    372\u001b[0m         model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;28mstr\u001b[39m(idx)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mtype\u001b[39m(module)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    374\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(model_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 375\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m     modules_config\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midx\u001b[39m\u001b[38;5;124m'\u001b[39m: idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(model_path), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mtype\u001b[39m(module)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m})\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodules.json\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fOut:\n",
      "File \u001b[0;32m~/anaconda/anaconda3/envs/coprovers/lib/python3.9/site-packages/sentence_transformers/models/Transformer.py:121\u001b[0m, in \u001b[0;36mTransformer.save\u001b[0;34m(self, output_path)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, output_path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(output_path)\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_bert_config.json\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fOut:\n",
      "File \u001b[0;32m~/anaconda/anaconda3/envs/coprovers/lib/python3.9/site-packages/transformers/modeling_utils.py:1072\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, save_config, state_dict, save_function, push_to_hub, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;66;03m# If we save using the predefined names, we can load using `from_pretrained`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m output_model_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, WEIGHTS_NAME)\n\u001b[0;32m-> 1072\u001b[0m \u001b[43msave_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_model_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1074\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel weights saved in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_model_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m push_to_hub:\n",
      "File \u001b[0;32m~/anaconda/anaconda3/envs/coprovers/lib/python3.9/site-packages/torch/serialization.py:380\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    379\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m--> 380\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    381\u001b[0m _legacy_save(obj, opened_file, pickle_module, pickle_protocol)\n",
      "File \u001b[0;32m~/anaconda/anaconda3/envs/coprovers/lib/python3.9/site-packages/torch/serialization.py:259\u001b[0m, in \u001b[0;36m_open_zipfile_writer_buffer.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:300] . unexpected pos 26478848 vs 26478736"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import losses\n",
    "\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1000, warmup_steps=100,\n",
    "          evaluator=evaluator,\n",
    "          output_path=str(MODEL_OUTPUT_PATH), \n",
    "          callback=status,\n",
    "         checkpoint_path=str(MODEL_SAVE_DIR), checkpoint_save_steps=10000, checkpoint_save_total_limit=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coprovers] *",
   "language": "python",
   "name": "conda-env-coprovers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
