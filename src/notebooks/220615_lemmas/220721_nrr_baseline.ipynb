{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3ef4c5-ed91-4507-a379-65f4f880db7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303e2eed-c9fd-4c72-a0cc-21cb592572cf",
   "metadata": {},
   "source": [
    "This attempts an extremely simple neural reranker experiment.  Motivated by \"Passage Re-ranking with BERT\", \n",
    "\n",
    "https://arxiv.org/abs/1901.04085\n",
    "https://github.com/nyu-dl/dl4marco-bert\n",
    "Passage Re-ranking with BERT\n",
    "Rodrigo Nogueira, Kyunghyun Cho\n",
    "\n",
    "We train a model where we start with the query [CLS] followed by the lemma, and then have a binary decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a3d55a-f00b-4f63-b9f8-7c05682eb6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615 9216\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import JSON\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "from enum import Enum\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import thundersvm\n",
    "\n",
    "from coprover import PROJ_ROOT, RSC_ROOT, PVSLIB_ROOT\n",
    "from featurizer import *\n",
    "\n",
    "DATA_ROOT = Path(PROJ_ROOT, \"data\", \"pvs\", \"pvslib\")\n",
    "json_files = list(DATA_ROOT.rglob(\"*.json\"))\n",
    "\n",
    "theory_files = list(DATA_ROOT.glob(\"*/*.json\"))\n",
    "proof_files = list(DATA_ROOT.glob(\"*/*/*.json\"))\n",
    "print(len(theory_files), len(proof_files))\n",
    "\n",
    "import pdb\n",
    "\n",
    "class VecTypes:\n",
    "    COUNT = 1\n",
    "    TFIDF = 2\n",
    "\n",
    "class TheoryBank:\n",
    "    def __init__(self, theory_files, \n",
    "                 vectorizer_type=VecTypes.COUNT,\n",
    "                 norm_vecs=True):\n",
    "        self.all_theories = collections.OrderedDict()\n",
    "        self.all_lemmas = collections.OrderedDict()\n",
    "        for json_fpath in tqdm(theory_files):\n",
    "            with open(json_fpath, 'r') as f:\n",
    "                theory_name = json_fpath.stem\n",
    "                doc_root = json.load(f)\n",
    "                theory = read_theory(doc_root)\n",
    "                self.all_theories[theory_name] = theory\n",
    "                self.all_lemmas.update(theory)\n",
    "        corpus = [\" \".join([str(y) for y in x]) for x in self.all_lemmas.values()]\n",
    "        self.names = list(self.all_lemmas.keys())\n",
    "        if vectorizer_type == VecTypes.COUNT:\n",
    "            self.vectorizer = CountVectorizer(stop_words=None, lowercase=False)\n",
    "        elif vectorizer_type == VecTypes.TFIDF:\n",
    "            self.vectorizer = TfidfVectorizer(stop_words=None, lowercase=False)\n",
    "        self.M = self.vectorizer.fit_transform(corpus).toarray()\n",
    "        # 1-norm\n",
    "        self.norm_vecs = norm_vecs\n",
    "        if self.norm_vecs:\n",
    "            self.M = self.M / np.linalg.norm(self.M, axis=1).reshape((self.M.shape[0], 1))\n",
    "        \n",
    "    def contains(self, name):\n",
    "        return name in self.names\n",
    "        \n",
    "    def query(self, qdocs, top_N=5):\n",
    "        \"\"\"\n",
    "        Expects a list of lists (docs x toks)\n",
    "        \"\"\"\n",
    "        q = self.vectorizer.transform(qdocs).toarray()\n",
    "        if self.norm_vecs:\n",
    "            q = q / np.linalg.norm(q, axis=0)\n",
    "        num_queries = len(qdocs)\n",
    "        S = self.M.dot(q.transpose())\n",
    "        sorted_idxes = np.argsort(S, axis=0)\n",
    "        if top_N is None:\n",
    "            max_idxes = sorted_idxes\n",
    "        else:\n",
    "            max_idxes = sorted_idxes[-top_N:, :]\n",
    "        \n",
    "        # Assemble list of names\n",
    "        titles = []\n",
    "        for qnum in range(len(qdocs)):\n",
    "            titles.append([self.names[idx] for idx in max_idxes[::-1, qnum]])\n",
    "        return titles\n",
    "    \n",
    "    def get(self, name, theories=None):\n",
    "        if theories is None:\n",
    "            theories = sorted(self.all_theories.keys())\n",
    "        for theory in theories:\n",
    "            if name in self.all_theories[theory]:\n",
    "                return self.all_theories[theory][name], theory\n",
    "        return None, None\n",
    "        # raise Exception(\"Could not identify name={} in theory set={}\".format(name, theories))\n",
    "    \n",
    "    def sample(self, rand_obj=None):\n",
    "        if rand_obj is None:\n",
    "            return np.random.choice(list(self.all_lemmas.values()))\n",
    "        else:\n",
    "            return rand_obj.choice(list(self.all_lemmas.values()))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "317f5f14-517c-4af8-957d-1276e59b4e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 615/615 [00:07<00:00, 82.25it/s]\n"
     ]
    }
   ],
   "source": [
    "theory_bank = TheoryBank(theory_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ed0708b-4312-4052-b870-8a88c9acdef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma queries cached, loading\n"
     ]
    }
   ],
   "source": [
    "# Now go through each of the proofs and collect the lemma requests\n",
    "\n",
    "LEMMA_OUTPUT_FPATH = Path(\"lemma_requests.json\")\n",
    "NAME = \"name\"\n",
    "STEP = \"step\"\n",
    "STATE = \"state\"\n",
    "CMD = \"command\"\n",
    "ARGS = \"args\"\n",
    "\n",
    "if LEMMA_OUTPUT_FPATH.exists():\n",
    "    # Load the lemma queries\n",
    "    print(\"Lemma queries cached, loading\")\n",
    "    with open(LEMMA_OUTPUT_FPATH, 'r') as f:\n",
    "        lemma_requests = json.load(f)\n",
    "else:\n",
    "    # Accumulate the lemma retrieval queries\n",
    "    tqdm_iter = tqdm(proof_files)\n",
    "    num_lemma_requests = 0\n",
    "    lemma_requests = []\n",
    "\n",
    "\n",
    "    for json_fpath in tqdm_iter:\n",
    "        name = Path(json_fpath).stem\n",
    "        sa_tuples = read_proof_session_lemmas(json_fpath)\n",
    "        if sa_tuples is None:\n",
    "            continue\n",
    "        for step_num, sa_tuple in enumerate(sa_tuples):\n",
    "            cmd, arg = sa_tuple[1], sa_tuple[2]\n",
    "            if arg is not None and isinstance(arg, str):\n",
    "                arg = arg.split(\"[\")[0]\n",
    "                if cmd in set([\"lemma\", \"rewrite\"]):\n",
    "                    # expr, theory = theory_bank.get(arg)\n",
    "                    num_lemma_requests += 1\n",
    "                    lemma_requests.append({\n",
    "                        STATE:  sa_tuple[0],\n",
    "                        CMD: cmd,\n",
    "                        ARGS: arg,\n",
    "                        NAME: name,\n",
    "                        STEP: step_num\n",
    "                    })\n",
    "                    tqdm_iter.set_description(\"# lemma requests={}\".format(num_lemma_requests))\n",
    "\n",
    "    # Save out the accumulated lemma requests to file                \n",
    "    with open(LEMMA_OUTPUT_FPATH, 'w') as f:\n",
    "        json.dump(lemma_requests, f)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52fd0078-5463-4fff-9aaf-ecb8f04f20fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train=12132, test=8089, total=20221\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split into train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "lemma_train, lemma_test = train_test_split(\n",
    "     lemma_requests, train_size=0.6, random_state=1337, shuffle=True, stratify=None)\n",
    "print(\"# train={}, test={}, total={}\".format(len(lemma_train), len(lemma_test), len(lemma_requests)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fecf4bce-e1dd-43d0-aa7d-f9303c94d6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize a query\n",
    "def make_statestr(state, consequents_only=False):\n",
    "    \"\"\" Given lemma state, converts into query string usable for theory bank\"\"\"\n",
    "    collecting = not(consequents_only)\n",
    "    toks = []\n",
    "    for tok in state:\n",
    "        if tok == \"consequents\":\n",
    "            collecting = True\n",
    "        elif collecting:\n",
    "            toks.append(str(tok))\n",
    "    return \" \".join(toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c028dd86-969e-46f8-896e-3a82a99fcf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 615/615 [00:02<00:00, 288.16it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 615/615 [00:02<00:00, 288.93it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 615/615 [00:02<00:00, 274.53it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 615/615 [00:02<00:00, 282.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make several theory banks\n",
    "count_tb_nonorm = TheoryBank(theory_files, vectorizer_type=VecTypes.COUNT, norm_vecs=False)\n",
    "tfidf_tb_nonorm = TheoryBank(theory_files, vectorizer_type=VecTypes.TFIDF, norm_vecs=False)\n",
    "count_tb_norm = TheoryBank(theory_files, vectorizer_type=VecTypes.COUNT, norm_vecs=True)\n",
    "tfidf_tb_norm = TheoryBank(theory_files, vectorizer_type=VecTypes.TFIDF, norm_vecs=True)\n",
    "\n",
    "experiments = ( (\"Count_NoNorm\", count_tb_nonorm), (\"TFIDF_NoNorm\", tfidf_tb_nonorm),\n",
    "              (\"Count_Norm\", count_tb_norm), (\"TFIDF_Norm\", tfidf_tb_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e9109b2-f193-4d50-bb90-50d11621ccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mrr(theory_bank):\n",
    "    retrieval_ranks = []\n",
    "    for req in tqdm(lemma_test):\n",
    "        gold = req[NAME]\n",
    "        if not(theory_bank.contains(gold)):\n",
    "            continue\n",
    "        state_str = make_statestr(req[STATE])\n",
    "        retrieved = theory_bank.query([state_str], top_N=None)\n",
    "        rank = retrieved[0].index(gold)\n",
    "        retrieval_ranks.append(rank)\n",
    "    mrr = np.mean([1/(rank + 1) for rank in retrieval_ranks])\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cf4a41-1921-4d4b-a88a-71a96459b581",
   "metadata": {},
   "source": [
    "# Supervised experiment.  \n",
    "For each of the training items, set up a featurization that consists of q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dd49377-6146-4b42-a4b9-4d0c5abc7359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "import pdb\n",
    "np_rng = default_rng(505)\n",
    "\n",
    "def assemble_data(lemma_src, tb, limit=100):\n",
    "    \"\"\"\n",
    "    Get one positive, exact match\n",
    "    One easy negative, random sample\n",
    "    One hard negative, random sample that is close\n",
    "    \"\"\"\n",
    "    for i in tqdm(range(limit)):\n",
    "        req = lemma_src[i]\n",
    "        query_state_str = make_statestr(req[STATE])\n",
    "        gold_lemma = tb.get(req[NAME])\n",
    "        gold_lemma_str = make_statestr(gold_lemma)\n",
    "        random_lemma_str = make_statestr(tb.sample())\n",
    "        \n",
    "        # Get the hard negative\n",
    "        S = tb.M.dot(gold_x.transpose())\n",
    "        sorted_idxes = np.argsort(S, axis=0)\n",
    "        hard_neg_idx = np_rng.integers(low=1, high=len(sorted_idxes), size=1)[0]\n",
    "        tb.\n",
    "        hard_neg_name = tb.names[hard_neg_idx]\n",
    "        hard_neg_lemma, _ = tb.get(hard_neg_name)\n",
    "        hard_neg_str = make_statestr(hard_neg_lemma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f41f7c3-cbeb-4797-bbd9-9746a2f7340c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                        | 0/10 [00:00<?, ?it/s]/tmp/ipykernel_31892/4227843498.py:93: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.random.choice(list(self.all_lemmas.values()))\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 16.10it/s]\n",
      "/tmp/ipykernel_31892/642839803.py:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X = np.array(X)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 16.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "# import sklearn.svm as svm\n",
    "# Try ThunderSVM, GPU accelerated SVC\n",
    "#num_train = 1000\n",
    "num_train = 10\n",
    "num_test = 10\n",
    "\n",
    "svc_results = collections.OrderedDict()\n",
    "for expname, theory_bank in experiments:\n",
    "    svc = svm.LinearSVC()\n",
    "    #svc = thundersvm.SVC()\n",
    "    train_X, train_Y = assemble_data(lemma_train, theory_bank, num_train)\n",
    "    test_X, test_Y = assemble_data(lemma_test, theory_bank, num_test)\n",
    "    print(\"Data assembled, fitting...\")\n",
    "    svc.fit(train_X, train_Y)\n",
    "    print(\"... testing\")\n",
    "    train_Yhat = svc.predict(train_X)\n",
    "    test_Yhat = svc.predict(test_X)\n",
    "    train_acc = np.sum(train_Yhat == train_Y) / train_X.shape[0]\n",
    "    test_acc = np.sum(test_Yhat == test_Y) / test_X.shape[0]\n",
    "    svc_results[expname] = {\n",
    "        \"train_acc\": train_acc,\n",
    "        \"test_acc\": test_acc\n",
    "    }\n",
    "    print(\"{}: train_acc={:.3f}, test_acc={:.3f}\".format(expname, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e7bd7-e947-4efe-a69c-40886b0a6500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coprovers] *",
   "language": "python",
   "name": "conda-env-coprovers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
