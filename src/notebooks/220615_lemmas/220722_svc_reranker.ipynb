{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f026ffb-5df8-4d6c-9004-932112ae5d24",
   "metadata": {},
   "source": [
    "# SVC Reranking Experiment\n",
    "\n",
    "Before we go full Transformer, let's see if we can use the classification result to act as a reranker instead.\n",
    "\n",
    "We develop a probabilistic SVC via a multiplicative state difference over TFIDF representations.  We then train a RBF kernel SVC with probabilistic reranking activated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebe72512-9250-486f-a352-010f0e47cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50e7c88f-61f6-4e6a-b606-103a75fedf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615 9216\n",
      "Lemma queries cached, loading\n",
      "# train=12132, test=8089, total=20221\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import JSON\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "from enum import Enum\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import thundersvm\n",
    "\n",
    "from coprover import PROJ_ROOT, RSC_ROOT, PVSLIB_ROOT\n",
    "from featurizer import *\n",
    "\n",
    "DATA_ROOT = Path(PROJ_ROOT, \"data\", \"pvs\", \"pvslib\")\n",
    "json_files = list(DATA_ROOT.rglob(\"*.json\"))\n",
    "\n",
    "theory_files = list(DATA_ROOT.glob(\"*/*.json\"))\n",
    "proof_files = list(DATA_ROOT.glob(\"*/*/*.json\"))\n",
    "print(len(theory_files), len(proof_files))\n",
    "\n",
    "import pdb\n",
    "\n",
    "class VecTypes:\n",
    "    COUNT = 1\n",
    "    TFIDF = 2\n",
    "\n",
    "class TheoryBank:\n",
    "    def __init__(self, theory_files, \n",
    "                 vectorizer_type=VecTypes.COUNT,\n",
    "                 norm_vecs=True):\n",
    "        self.all_theories = collections.OrderedDict()\n",
    "        self.all_lemmas = collections.OrderedDict()\n",
    "        for json_fpath in tqdm(theory_files):\n",
    "            with open(json_fpath, 'r') as f:\n",
    "                theory_name = json_fpath.stem\n",
    "                doc_root = json.load(f)\n",
    "                theory = read_theory(doc_root)\n",
    "                self.all_theories[theory_name] = theory\n",
    "                self.all_lemmas.update(theory)\n",
    "        corpus = [\" \".join([str(y) for y in x]) for x in self.all_lemmas.values()]\n",
    "        self.names = list(self.all_lemmas.keys())\n",
    "        if vectorizer_type == VecTypes.COUNT:\n",
    "            self.vectorizer = CountVectorizer(stop_words=None, lowercase=False)\n",
    "        elif vectorizer_type == VecTypes.TFIDF:\n",
    "            self.vectorizer = TfidfVectorizer(stop_words=None, lowercase=False)\n",
    "        self.M = self.vectorizer.fit_transform(corpus).toarray()\n",
    "        # 1-norm\n",
    "        self.norm_vecs = norm_vecs\n",
    "        if self.norm_vecs:\n",
    "            self.M = self.M / np.linalg.norm(self.M, axis=1).reshape((self.M.shape[0], 1))\n",
    "        \n",
    "    def contains(self, name):\n",
    "        return name in self.names\n",
    "        \n",
    "    def query(self, qdocs, top_N=5):\n",
    "        \"\"\"\n",
    "        Expects a list of lists (docs x toks)\n",
    "        \"\"\"\n",
    "        q = self.vectorizer.transform(qdocs).toarray()\n",
    "        if self.norm_vecs:\n",
    "            q = q / np.linalg.norm(q, axis=0)\n",
    "        num_queries = len(qdocs)\n",
    "        S = self.M.dot(q.transpose())\n",
    "        sorted_idxes = np.argsort(S, axis=0)\n",
    "        if top_N is None:\n",
    "            max_idxes = sorted_idxes\n",
    "        else:\n",
    "            max_idxes = sorted_idxes[-top_N:, :]\n",
    "        \n",
    "        # Assemble list of names\n",
    "        titles = []\n",
    "        for qnum in range(len(qdocs)):\n",
    "            titles.append([self.names[idx] for idx in max_idxes[::-1, qnum]])\n",
    "        return titles, max_idxes.ravel()\n",
    "    \n",
    "    def get(self, name, theories=None):\n",
    "        if theories is None:\n",
    "            theories = sorted(self.all_theories.keys())\n",
    "        for theory in theories:\n",
    "            if name in self.all_theories[theory]:\n",
    "                return self.all_theories[theory][name], theory\n",
    "        return None, None\n",
    "        # raise Exception(\"Could not identify name={} in theory set={}\".format(name, theories))\n",
    "    \n",
    "    def sample(self, rand_obj=None):\n",
    "        if rand_obj is None:\n",
    "            return np.random.choice(list(self.all_lemmas.values()))\n",
    "        else:\n",
    "            return rand_obj.choice(list(self.all_lemmas.values()))\n",
    "        \n",
    "\n",
    "        # Now go through each of the proofs and collect the lemma requests\n",
    "\n",
    "LEMMA_OUTPUT_FPATH = Path(\"lemma_requests.json\")\n",
    "NAME = \"name\"\n",
    "STEP = \"step\"\n",
    "STATE = \"state\"\n",
    "CMD = \"command\"\n",
    "ARGS = \"args\"\n",
    "\n",
    "if LEMMA_OUTPUT_FPATH.exists():\n",
    "    # Load the lemma queries\n",
    "    print(\"Lemma queries cached, loading\")\n",
    "    with open(LEMMA_OUTPUT_FPATH, 'r') as f:\n",
    "        lemma_requests = json.load(f)\n",
    "else:\n",
    "    # Accumulate the lemma retrieval queries\n",
    "    tqdm_iter = tqdm(proof_files)\n",
    "    num_lemma_requests = 0\n",
    "    lemma_requests = []\n",
    "\n",
    "\n",
    "    for json_fpath in tqdm_iter:\n",
    "        name = Path(json_fpath).stem\n",
    "        sa_tuples = read_proof_session_lemmas(json_fpath)\n",
    "        if sa_tuples is None:\n",
    "            continue\n",
    "        for step_num, sa_tuple in enumerate(sa_tuples):\n",
    "            cmd, arg = sa_tuple[1], sa_tuple[2]\n",
    "            if arg is not None and isinstance(arg, str):\n",
    "                arg = arg.split(\"[\")[0]\n",
    "                if cmd in set([\"lemma\", \"rewrite\"]):\n",
    "                    # expr, theory = theory_bank.get(arg)\n",
    "                    num_lemma_requests += 1\n",
    "                    lemma_requests.append({\n",
    "                        STATE:  sa_tuple[0],\n",
    "                        CMD: cmd,\n",
    "                        ARGS: arg,\n",
    "                        NAME: name,\n",
    "                        STEP: step_num\n",
    "                    })\n",
    "                    tqdm_iter.set_description(\"# lemma requests={}\".format(num_lemma_requests))\n",
    "\n",
    "    # Save out the accumulated lemma requests to file                \n",
    "    with open(LEMMA_OUTPUT_FPATH, 'w') as f:\n",
    "        json.dump(lemma_requests, f)\n",
    "        \n",
    "\n",
    "        \n",
    "# Split into train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "lemma_train, lemma_test = train_test_split(\n",
    "     lemma_requests, train_size=0.6, random_state=1337, shuffle=True, stratify=None)\n",
    "print(\"# train={}, test={}, total={}\".format(len(lemma_train), len(lemma_test), len(lemma_requests)))\n",
    "\n",
    "\n",
    "# Vectorize a query\n",
    "def make_statestr(state, consequents_only=False):\n",
    "    \"\"\" Given lemma state, converts into query string usable for theory bank\"\"\"\n",
    "    collecting = not(consequents_only)\n",
    "    toks = []\n",
    "    for tok in state:\n",
    "        if tok == \"consequents\":\n",
    "            collecting = True\n",
    "        elif collecting:\n",
    "            toks.append(str(tok))\n",
    "    return \" \".join(toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fb57f23-7d9e-413c-b099-16a1e3bda679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 615/615 [00:02<00:00, 259.21it/s]\n"
     ]
    }
   ],
   "source": [
    "tfidf_tb_nonorm = TheoryBank(theory_files, vectorizer_type=VecTypes.TFIDF, norm_vecs=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60f9e170-1315-4126-b978-dbcc96185068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "import pdb\n",
    "np_rng = default_rng(505)\n",
    "\n",
    "def featurize_mult(x1, x2):\n",
    "    return np.multiply(x1, x2).ravel()\n",
    "\n",
    "def featurize(x1, x2):\n",
    "    return np.abs(x1 - x2).ravel()\n",
    "\n",
    "\n",
    "def assemble_data(lemma_src, tb, limit=100):\n",
    "    \"\"\"\n",
    "    Get one positive, exact match\n",
    "    One easy negative, random sample\n",
    "    One hard negative, random sample that is close\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in tqdm(range(limit)):\n",
    "        req = lemma_src[i]\n",
    "        state_str = make_statestr(req[STATE])\n",
    "        q = tb.vectorizer.transform([state_str]).toarray()\n",
    "        gold_lemma = tb.get(req[NAME])\n",
    "        gold_x = tb.vectorizer.transform([make_statestr(gold_lemma)]).toarray()\n",
    "        random_x = tb.vectorizer.transform([make_statestr(tb.sample())]).toarray()      \n",
    "        x_pos = featurize(q, gold_x)\n",
    "        x_neg = featurize(q, random_x)\n",
    "        \n",
    "        # Get the hard negative\n",
    "        S = tb.M.dot(gold_x.transpose())\n",
    "        sorted_idxes = np.argsort(S, axis=0)\n",
    "        hard_neg_idx = np_rng.integers(low=1, high=len(sorted_idxes), size=1)[0]\n",
    "        x_hard_neg = featurize(q, tb.M[hard_neg_idx].reshape((1, -1)))\n",
    "        # pdb.set_trace()\n",
    "        X.append(x_pos)\n",
    "        X.append(x_neg)\n",
    "        X.append(x_hard_neg)\n",
    "        Y.append(1)\n",
    "        Y.append(-1)\n",
    "        Y.append(-1)        \n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5e9691a-cfc1-4a9a-b40d-01c0df67ea00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug status=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                           | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_42565/2098055782.py:93: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.random.choice(list(self.all_lemmas.values()))\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:16<00:00, 59.64it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 58.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file=models/rbf.mult.debug.joblib exists, avoiding training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from pprint import pprint\n",
    "from joblib import dump, load\n",
    "# joblib is preferred for sklearn\n",
    "\n",
    "\n",
    "#num_train = 1000\n",
    "if DEBUG:\n",
    "    num_train = 1000\n",
    "    num_test = 100\n",
    "    suffix = \"debug\"\n",
    "else:\n",
    "    num_train = len(lemma_train)\n",
    "    num_test = len(lemma_test)\n",
    "    suffix = \"full\"\n",
    "\n",
    "print(\"Debug status={}\".format(DEBUG))\n",
    "\n",
    "theory_bank = tfidf_tb_nonorm\n",
    "expname = \"tfidf_nonorm\"\n",
    "\n",
    "MODEL_DIR = Path(\"models\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_X, train_Y = assemble_data(lemma_train, theory_bank, num_train)\n",
    "test_X, test_Y = assemble_data(lemma_test, theory_bank, num_test)\n",
    "svm_name = \"RBF\"\n",
    "svc = svm.SVC(kernel='rbf')\n",
    "model_fpath = Path(MODEL_DIR, 'rbf.l1.{}.joblib'.format(suffix))\n",
    "if model_fpath.exists():\n",
    "    print(\"Model file={} exists, avoiding training\".format(model_fpath))\n",
    "    svc = load(model_fpath)\n",
    "else:\n",
    "    print(\"Training model, will save to {}\".format(model_fpath))\n",
    "    svc.fit(train_X, train_Y)\n",
    "    print(\"... testing\")\n",
    "    train_Yhat = svc.predict(train_X)\n",
    "    test_Yhat = svc.predict(test_X)\n",
    "    train_acc = np.sum(train_Yhat == train_Y) / train_X.shape[0]\n",
    "    test_acc = np.sum(test_Yhat == test_Y) / test_X.shape[0]\n",
    "    model_result = {\n",
    "        \"exp_name\": expname,\n",
    "        \"svc_type\": svm_name,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"test_acc\": test_acc\n",
    "    }\n",
    "    pprint(model_result)\n",
    "    dump(svc, model_fpath)\n",
    "    df = pd.DataFrame([model_result])\n",
    "    df.to_csv(Path(MODEL_DIR, \"{}.results.csv\".format(model_fpath.stem)), header=True, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f5313c7f-a1de-41f8-a75e-446ed5b8da14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# processed = 405: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [02:09<00:00,  7.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Now compute MRR, using the TFIDF to compute the initial rankings, and then to \n",
    "# rerank according to the SVC decision_function\n",
    "\n",
    "theory_bank = tfidf_tb_nonorm\n",
    "top_N=100\n",
    "orig_retrieval_ranks = []\n",
    "rerank_retrieval_ranks = []\n",
    "if DEBUG:\n",
    "    tqdm_iter = tqdm(lemma_test[0:1000])    \n",
    "else:\n",
    "    tqdm_iter = tqdm(lemma_test)\n",
    "for req in tqdm_iter:\n",
    "    gold = req[NAME]\n",
    "    if not(theory_bank.contains(gold)):\n",
    "        continue\n",
    "    state_str = make_statestr(req[STATE])\n",
    "    retrieved_sets, retrieved_idxes = theory_bank.query([state_str], top_N=top_N)\n",
    "    retrieved_titles = retrieved_sets[0]\n",
    "    q = theory_bank.vectorizer.transform([state_str]).toarray()\n",
    "    if gold not in retrieved_titles:\n",
    "        continue\n",
    "    orig_rank = retrieved_titles.index(gold)\n",
    "    orig_retrieval_ranks.append(orig_rank)\n",
    "    tqdm_iter.set_description(\"# processed = {}\".format(len(orig_retrieval_ranks)))\n",
    "    # Develop the subset of the test set corresponding to the retrieved idxes\n",
    "    if True:\n",
    "        candidates_X = theory_bank.M[retrieved_idxes]\n",
    "        candidate_scores = svc.decision_function(candidates_X)\n",
    "        sorted_idxes = np.argsort(candidate_scores)\n",
    "        sorted_titles = np.array(retrieved_titles)[sorted_idxes[::-1]]\n",
    "        sorted_rank = list(sorted_titles).index(gold)\n",
    "        rerank_retrieval_ranks.append(sorted_rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0494b690-63e1-482a-9eb0-e6aebe0f41d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig MRR=0.15853, Rerank MRR=0.06236\n"
     ]
    }
   ],
   "source": [
    "orig_mrr = np.mean([1/(rank + 1) for rank in orig_retrieval_ranks])\n",
    "rerank_mrr = np.mean([1/(rank + 1) for rank in rerank_retrieval_ranks])\n",
    "print(\"Orig MRR={:.5f}, Rerank MRR={:.5f}\".format(orig_mrr, rerank_mrr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "59808253-62df-4e18-84da-6683e52c434b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.19770081, -0.95177405, -0.95177405,  1.63318344, -0.91961961,\n",
       "        0.10431134,  1.12825163,  0.76285118,  0.57942477,  0.65035609,\n",
       "       -0.13755259,  0.40447642, -0.02975411, -0.14244391,  0.27508748,\n",
       "       -0.11947358,  0.73001479,  0.5037207 ,  0.18088607,  0.37293112,\n",
       "       -0.55328429,  0.3966337 , -0.22475029, -0.02698504,  0.71757191,\n",
       "        0.57953026, -0.37666171,  0.12112822,  0.57188458, -0.34035353,\n",
       "       -0.0336449 ,  0.20871212, -0.2444568 ,  0.50590581,  0.46242202,\n",
       "        0.30623153,  0.41565708,  0.25600512, -0.09187527, -0.14440487,\n",
       "        0.46237381, -0.4449498 ,  0.57265977, -0.12741893,  0.78081296,\n",
       "        0.22356283, -0.17469775,  0.10902293, -0.0312339 ,  0.1741706 ,\n",
       "        0.15882305,  0.32409993, -0.30192729,  0.29633547,  0.42224225,\n",
       "       -0.14447161,  0.49408204, -0.35431708,  0.38898623, -0.32333434,\n",
       "        0.15774202,  0.13256068,  0.00592956,  0.28545738, -0.24997302,\n",
       "        0.16453363,  0.06322592, -0.33819694, -0.26103445, -0.26571769,\n",
       "        0.09754841,  0.3264052 ,  0.31092955,  0.08584179,  0.18672898,\n",
       "        0.27013664,  3.00287436,  3.00287436,  3.00287436,  3.00287436,\n",
       "        3.00287436,  3.00287436,  3.00287436,  3.00287436,  3.00287436,\n",
       "        3.00287436,  3.00287436,  3.00287436,  3.00287436,  3.00287436,\n",
       "        3.00287436,  3.00287436,  3.00287436,  3.00287436,  3.00287436,\n",
       "        3.00287436,  3.00287436,  3.00287436,  3.00287436,  3.00287436])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "94e09f1e-c3be-49df-ad36-6cc76b60fcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  4, 20, 41, 26, 57, 29, 67, 59, 52, 69, 68, 64, 32, 22, 46,\n",
       "       55, 39, 13, 10, 43, 15, 38, 30, 48, 12, 23, 62, 66, 73, 70,  5, 47,\n",
       "       27, 61, 60, 50, 65, 49, 18, 74, 31, 45, 37, 75, 14, 63, 53, 35, 72,\n",
       "       51, 71, 19, 58, 21, 11, 36, 54, 40, 34, 56, 17, 33, 28, 42,  8, 25,\n",
       "        9, 24, 16,  7, 44,  6,  0,  3, 97, 96, 95, 94, 93, 92, 91, 90, 89,\n",
       "       88, 98, 87, 85, 84, 83, 82, 81, 80, 79, 78, 77, 76, 86, 99])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(candidate_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "988a1a8e-fe80-4fde-b2fd-7158f64b97d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4624220157395511"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_scores[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a82875f1-b8c7-4395-bc3a-13da318ebcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = svc.predict(candidates_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "750d1862-fd16-4782-88ef-91914615f745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013212057592558602"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.multiply(candidate_scores, np.array([1/x for x in range(1, 101, 1)]))\n",
    "W[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c9e96f4f-5b5a-4bd1-a091-a6a8a03befe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.argsort(W)).index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267d7809-35e1-437f-96f0-c45318ce6624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coprovers] *",
   "language": "python",
   "name": "conda-env-coprovers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
